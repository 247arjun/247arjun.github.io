---
layout: post
title: "Testing 1... 2..."
date: 2026-01-17
---

# Testing 1... 2...

In the year of our AI overlord, 2026, the world of software engineering is in flux. Big Tech, and the tech industry in general, has embraced LLM-assisted coding as a mainstream technology. There are specific GenAI models trained and released that are recognized as being purpose-built for code (e.g. OpenAI gpt-5.2-codex) as well as systems that use these to help devs code faster (GitHub Copilot, Claude Code etc.). All this has opened the floodgates (to a reasonable degree) for code generation velocity. 

Steve Yegge in [his article](https://steve-yegge.medium.com/six-new-tips-for-better-coding-with-agents-d4e9c86e42a9), which itself references [Joel Spolsky's earlier article](https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/), talks about the new shortened lifespan of code and how "for all the code I write, I now expect to throw it away in about a year, to be replaced by something better."

Now, new code written faster is great but unhelpful if it doesn't satisfy the traditional "itys" (security, reliability, extensibility, scalability etc.) of software engineering. 

> "... time is a valuable thing  
> Watch it fly by as the pendulum swings" - Linkin Park (In the End, Hybrid Theory)

The way this has historically been measured and enforced is with tests. Testing in software engineering at Microsoft has, in the past decade, swung from having its own dedicated discipline (RIP the SDE-T role, where SDETs were tasked with developing, maintaining and executing test frameworks and test suites) to being merged into the SDE role (which itself has evolved to DevOps). The past velocity of releases (remember when there was a new Windows OS release every 4 years) afforded the dedicated time and focus to this work, which in an ever-changing online services environment doesn't translate as well. 

> "доверяй, но проверяй" ("Trust, but verify") - Russian proverb 

Now, looking ahead at the anticipated increased velocity with AI, the pendulum may be swinging back. As AI frees up dev cycles from reducing the overhead associated with writing production code, that time is being spent in testing/evaluating this code. Devs are being flooded with AI-generated pull requests and one of the few deterministic and scalable levers they have to ensure validation is tests. Having test coverage, building on Test-Driven Development (TDD) is a good approach here.

The other side of the equation is the growing Agent ecosystem, where software with deterministic execution flows is being replaced by model-inferenced variable execution flow. Testing these systems is trickier, since the inherent non-determinism of LLMs means you will have to define your tests at the level of granularity to ensure a pass rate that is acceptable, accounting for the variability in the model behavior.

Anthropic released an article, [demystifying evals](https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents), that touches on this in greater depth and is an excellent primer on evals in general.

All this to say, testing is a skill that is back in demand; a skill that waned a decade ago in favor of software development, but now back in demand as the critical thinking skills associated with that discipline are once again relevant.
